{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0337e8-c72c-442e-a0c0-6021d07a660f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!conda install -c rapidsai -c nvidia -c conda-forge cuml=24.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19fd70c-1712-44ce-867c-ffc217ed5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyamg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981bc3ca-ed1f-4ba7-85ed-f345314e8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61be43c0-cfd4-466d-a2a4-faa9eb00b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 01:36:14.814561: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 01:36:15.071744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746819375.177699  335077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746819375.212401  335077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-10 01:36:15.459588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import random\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from cuml.manifold import UMAP as cuUMAP\n",
    "from cuml.decomposition import PCA as cuPCA, TruncatedSVD as cuTruncatedSVD\n",
    "from cuml.neighbors import NearestNeighbors as cuNearestNeighbors\n",
    "\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5f230a-7e21-4af9-97da-c4562555dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75a13-4536-4483-bfef-8a57acdb5f8e",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991c746f-990a-4438-8bb7-37f2f84e1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickled/banfake_string.pickle', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "with open('./pickled/bn_stopwords.pickle', 'rb') as f:\n",
    "    bn_stopwords = pickle.load(f)\n",
    "\n",
    "num_topics = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68a3b31-74c3-4d18-ba07-b65d85a96733",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6adcbd61-1276-4d05-9b08-147ff54265ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs= docs[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfd67c-86f1-45a8-94cb-b93e7977cc00",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "444763ec-e146-41bf-9b19-c7278b46ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b971960-f6c5-4972-9629-40e3a8f67a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(docs, model_path, dim_reduc=None, nr_topics=num_topics):\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=bn_stopwords,\n",
    "        ngram_range=(1, 1),\n",
    "        tokenizer=bengali_tokenizer\n",
    "    )\n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    # Generate embeddings on GPU\n",
    "    embedder = SentenceTransformer(model_path, device='cuda')\n",
    "    embeddings = embedder.encode(docs, device='cuda').astype(np.float32)\n",
    "    gdf_data = cp.asarray(embeddings)\n",
    "\n",
    "    \n",
    "    if dim_reduc == 'umap':\n",
    "        reducer = cuUMAP(n_components=10, metric='cosine')\n",
    "    elif dim_reduc == 'pca':\n",
    "        reducer = cuPCA(n_components=128)\n",
    "    elif dim_reduc == 'svd':\n",
    "        reducer = cuTruncatedSVD(n_components=100)\n",
    "    else:\n",
    "        reducer = None\n",
    "\n",
    "    if reducer:\n",
    "        gdf_reduced = reducer.fit_transform(gdf_data)\n",
    "    else:\n",
    "        gdf_reduced = gdf_data\n",
    "\n",
    "    \n",
    "    nn = cuNearestNeighbors(n_neighbors=15, metric=\"cosine\")\n",
    "    nn.fit(gdf_reduced)\n",
    "    distances, indices = nn.kneighbors(gdf_reduced)\n",
    "    \n",
    "\n",
    "    indices = indices.get()\n",
    "    distances = distances.get()\n",
    "    n_samples = indices.shape[0]\n",
    "    \n",
    "    # Clean up\n",
    "    del gdf_data, gdf_reduced\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    for i in range(n_samples):\n",
    "        for j in range(1, 10):  \n",
    "            idx = indices[i, j]\n",
    "            sim = 1 - distances[i, j]\n",
    "            rows.extend([i, idx])\n",
    "            cols.extend([idx, i])\n",
    "            data.extend([sim, sim])\n",
    "\n",
    "    affinity = coo_matrix((data, (rows, cols)), shape=(n_samples, n_samples)).tocsr()\n",
    "    \n",
    "    # Clean up\n",
    "    del rows, cols, data, indices, distances\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    clustering = SpectralClustering(\n",
    "        n_clusters=nr_topics,\n",
    "        eigen_solver='amg',\n",
    "        affinity='precomputed',\n",
    "        random_state=42\n",
    "    ).fit(affinity)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedder,\n",
    "        umap_model=reducer,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        nr_topics=nr_topics,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "    if isinstance(embeddings, cp.ndarray):\n",
    "        embeddings = embeddings.get()\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings=embeddings, y=labels)\n",
    "\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76372348-44f3-42fb-b07e-a0d0390c8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_words(topic_words, docs):\n",
    "    \n",
    "    tokenized_docs = [bengali_tokenizer(doc) for doc in docs]\n",
    "    texts = [doc for doc in tokenized_docs if len(doc) > 0]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    if texts and dictionary:\n",
    "        \n",
    "        coherence_c_v = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "        coherence_npmi = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "        \n",
    "        results['coherence_c_v'] = coherence_c_v\n",
    "        results['coherence_npmi'] = coherence_npmi\n",
    "    \n",
    "    octis = {\n",
    "        'topics': topic_words  \n",
    "    }\n",
    "    \n",
    "    td = TopicDiversity(topk=10)\n",
    "    irbo = InvertedRBO(topk=10)\n",
    "    \n",
    "    results['topic_diversity'] = td.score(octis)\n",
    "    results['IRBO'] = irbo.score(octis)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9d4dc-4158-44b8-af59-0a74f2edee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "topic_model = topic_modeling(docs=docs,\n",
    "                             model_path=\"shihab17/bangla-sentence-transformer\", \n",
    "                             dim_reduc='umap')\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f92557-e05a-4e4c-b5a0-0a41d43ff739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = topic_model.get_topic_info()\n",
    "topic_words = df['Representation'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400dd40-b157-4cec-8738-d12853254737",
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in topic_words:\n",
    "    print(f'{topic_words.index(words)} : {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73e5c1-5d13-4cbb-bc01-eebd98643d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81710a-b436-43d5-8bfb-94b7d21e3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_topic_words(topic_words, docs)\n",
    "results['runtime'] = runtime\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af428902-868c-4c27-9a9f-20685276bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddf4b5-170a-4c79-bb99-525c52797650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887d56b-6755-4678-9d30-56bd5ddcc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813e152-4b21-407b-8b7b-a3318a7e47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10cc96-fbeb-41c6-9f28-24ff8b47b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5541ed-9cf2-42e7-bff5-8db022a326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 2  \n",
    "sampling_rate = 44100\n",
    "frequency = 440.0 \n",
    "t = np.linspace(0, duration, int(sampling_rate * duration), False)\n",
    "wave = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "audio = ipd.Audio(wave, rate=sampling_rate, autoplay=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c183b3-1a0a-4ef3-84b9-4396378915eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
