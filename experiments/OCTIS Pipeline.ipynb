{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e00e188-69c0-4b18-be03-1a4960c14c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import corpora\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "import numpy as np\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "import IPython.display as ipd\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.models.ProdLDA import ProdLDA\n",
    "from octis.models.NeuralLDA import NeuralLDA\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46326bd-04d0-4b12-8ba7-d810fdc420ff",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6a018b-bd79-4535-a5a5-d21121b14f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickled/banfake_tokenized.pickle', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd807cc2-2c6c-4154-a1d0-9c1152b28373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 130227\n"
     ]
    }
   ],
   "source": [
    "out_folder = './data/banfake'\n",
    "sanitized_docs = []\n",
    "for doc in docs:\n",
    "    sanitized_doc = [token.replace('\\t', ' ') for token in doc]\n",
    "    sanitized_docs.append(sanitized_doc)\n",
    "\n",
    "with open(f\"{out_folder}/corpus.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for doc in sanitized_docs:\n",
    "        doc_text = \" \".join(doc)\n",
    "        if doc_text.strip():  \n",
    "            writer.writerow([doc_text, \"train\"])\n",
    "\n",
    "vocab = sorted({tok for doc in sanitized_docs for tok in doc})\n",
    "with open(f\"{out_folder}/vocabulary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0dded0-e469-4f7d-8de1-9fbe0a0b8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9648377a-373b-4ef6-b0e9-513b8be714f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_words(topic_words, texts):\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    if texts and dictionary:\n",
    "        \n",
    "        coherence_c_v = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "        coherence_npmi = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "        \n",
    "        results['coherence_c_v'] = coherence_c_v\n",
    "        results['coherence_npmi'] = coherence_npmi\n",
    "    \n",
    "    octis = {\n",
    "        'topics': topic_words  \n",
    "    }\n",
    "    \n",
    "    td = TopicDiversity(topk=10)\n",
    "    irbo = InvertedRBO(topk=10)\n",
    "    \n",
    "    results['topic_diversity'] = td.score(octis)\n",
    "    results['IRBO'] = irbo.score(octis)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fada173-eec7-4cd0-b857-3c616b589b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"./data/banfake\")\n",
    "num_topics = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a427b9-8a62-4f94-8713-7e9c048027b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.2 GiB for an array with shape (48675, 130227) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m prodlda \u001b[38;5;241m=\u001b[39m ProdLDA(\n\u001b[1;32m      2\u001b[0m     num_topics\u001b[38;5;241m=\u001b[39mnum_topics,\n\u001b[1;32m      3\u001b[0m     use_partitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m prodlda_output \u001b[38;5;241m=\u001b[39m \u001b[43mprodlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m runtime \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/basic/lib/python3.10/site-packages/octis/models/ProdLDA.py:21\u001b[0m, in \u001b[0;36mProdLDA.train_model\u001b[0;34m(self, dataset, hyperparameters, top_words)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, hyperparameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, top_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/basic/lib/python3.10/site-packages/octis/models/pytorchavitm/AVITM.py:92\u001b[0m, in \u001b[0;36mAVITM.train_model\u001b[0;34m(self, dataset, hyperparameters, top_words)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_vocabulary()\n\u001b[1;32m     91\u001b[0m     data_corpus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mget_corpus()]\n\u001b[0;32m---> 92\u001b[0m     x_train, input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m avitm_model\u001b[38;5;241m.\u001b[39mAVITM_model(\n\u001b[1;32m     95\u001b[0m     input_size\u001b[38;5;241m=\u001b[39minput_size, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_topics\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     96\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     topic_prior_variance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprior_variance\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_partitions:\n",
      "File \u001b[0;32m~/miniconda3/envs/basic/lib/python3.10/site-packages/octis/models/pytorchavitm/AVITM.py:168\u001b[0m, in \u001b[0;36mAVITM.preprocess\u001b[0;34m(vocab, train, test, validation)\u001b[0m\n\u001b[1;32m    166\u001b[0m idx2token \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m vec\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    167\u001b[0m X_train \u001b[38;5;241m=\u001b[39m vec\u001b[38;5;241m.\u001b[39mtransform(train)\n\u001b[0;32m--> 168\u001b[0m train_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mBOWDataset(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, idx2token)\n\u001b[1;32m    169\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(idx2token\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/basic/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1056\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1056\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/basic/lib/python3.10/site-packages/scipy/sparse/_base.py:1287\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 47.2 GiB for an array with shape (48675, 130227) and data type int64"
     ]
    }
   ],
   "source": [
    "prodlda = ProdLDA(\n",
    "    num_topics=num_topics,\n",
    "    use_partitions=False,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "prodlda_output = prodlda.train_model(dataset)\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "topic_words = prodlda_output['topics']\n",
    "\n",
    "for idx, topic in enumerate(topic_words):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718813b-1233-426a-9d46-b82bf52e7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_topic_words(topic_words, docs)\n",
    "results['runtime'] = runtime\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814996dd-88b9-433e-9243-a415ab99a4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_lda = NeuralLDA(\n",
    "    num_topics=num_topics,\n",
    "    use_partitions=False,\n",
    "    batch_size=8\n",
    ")\n",
    "start_time = time.time()\n",
    "nlda_output = neural_lda.train_model(dataset)\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "topic_words = nlda_output['topics']\n",
    "\n",
    "for idx, topic in enumerate(topic_words):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb87e3-49eb-4bf7-9c7d-74ea9ce0aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_topic_words(topic_words, docs)\n",
    "results['runtime'] = runtime\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e86d7c-8ca7-46e6-86a9-f3ec275ef081",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 2  \n",
    "sampling_rate = 44100\n",
    "frequency = 440.0 \n",
    "t = np.linspace(0, duration, int(sampling_rate * duration), False)\n",
    "wave = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "audio = ipd.Audio(wave, rate=sampling_rate, autoplay=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9e95-50af-40a0-96e6-d64f550358b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
