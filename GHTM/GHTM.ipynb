{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c6face-f357-45f9-8cbe-692b6759844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.decomposition import NMF\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "from torch_geometric.utils import dropout_edge\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ClusterGCNConv\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import random\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfa67d4-6838-44d9-9ed0-553a8c45ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a760e04-8dfc-4dd6-af7c-340aef8d9de2",
   "metadata": {},
   "source": [
    "## GloVe and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4dbf08-cb72-4353-a3a8-37341c82e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('./model/bn_glove.39M.300d.txt', encoding='utf8') as f:\n",
    "    for L in f:\n",
    "        parts = L.rstrip().split()\n",
    "        glove[parts[0]] = np.array(parts[1:], dtype=float)\n",
    "emb_dim = len(next(iter(glove.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692713c7-b2be-434f-baad-f73ad4538c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/nctbtext_processed.pickle', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "random.shuffle(docs)\n",
    "\n",
    "num_topics = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cf3a2-e8ae-402a-bb0e-5946de803d1d",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f227d95-e281-4322-9e6f-73a8cb1006eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3a8442-0aab-4b19-b436-b60d4ac8c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_words(topic_words, texts):\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    if texts and dictionary:\n",
    "        \n",
    "        coherence_c_v = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "        coherence_npmi = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "        \n",
    "        results['coherence_c_v'] = coherence_c_v\n",
    "        results['coherence_npmi'] = coherence_npmi\n",
    "    \n",
    "    octis = {\n",
    "        'topics': topic_words  \n",
    "    }\n",
    "    \n",
    "    td = TopicDiversity(topk=10)\n",
    "    irbo = InvertedRBO(topk=10)\n",
    "    \n",
    "    results['topic_diversity'] = td.score(octis)\n",
    "    results['IRBO'] = irbo.score(octis)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f0a64d-74fb-4f71-a71a-f6f4ae39ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn_graph(embeddings: np.ndarray, k: int):\n",
    "    \"\"\"\n",
    "    embeddings: [num_nodes, 300] numpy array of text embeddings\n",
    "    k: number of neighbors for k-NN\n",
    "    returns: edge_index tensor for PyG\n",
    "    \"\"\"\n",
    "    A = kneighbors_graph(embeddings, n_neighbors=k, mode='distance', metric='cosine', n_jobs=-1, include_self=True)      \n",
    "\n",
    "    A = A.tocoo()\n",
    "    edge_index = torch.tensor([A.row, A.col], dtype=torch.long)\n",
    "    x = torch.tensor(embeddings, dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7feecee7-4228-475a-acfc-da8da6e226ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        dropout: float = 0.4,\n",
    "        edge_dropout: float = 0.2,      \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_drop = nn.Dropout(p=dropout)            \n",
    "    \n",
    "        self.convs = nn.ModuleList([\n",
    "            ClusterGCNConv(in_dim, hidden_dim),\n",
    "            ClusterGCNConv(hidden_dim, hidden_dim),\n",
    "            ClusterGCNConv(hidden_dim, hidden_dim),\n",
    "            ClusterGCNConv(hidden_dim, out_dim),\n",
    "        ])\n",
    "    \n",
    "        self.norms = nn.ModuleList([\n",
    "            GraphNorm(hidden_dim),\n",
    "            GraphNorm(hidden_dim),\n",
    "            GraphNorm(hidden_dim),\n",
    "            GraphNorm(out_dim),\n",
    "        ])                                                \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            edge_index, _ = dropout_edge(\n",
    "                edge_index, p=self.edge_dropout, training=self.training\n",
    "            )                                             \n",
    " \n",
    "            h = conv(x, edge_index)\n",
    "\n",
    "            h = self.norms[i](h)\n",
    "\n",
    "            if i != len(self.convs) - 1:          \n",
    "                h = F.relu(h)\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "            if h.shape == x.shape:\n",
    "                h = h + x                                            \n",
    "\n",
    "            x = h\n",
    "\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d965b28-2fc3-4ae2-9239-f1773c4b759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_loader(data, num_clusters, batch_size):\n",
    "    \"\"\"\n",
    "    data: PyG Data object\n",
    "    num_clusters: into how many partitions to split the full graph\n",
    "    batch_size: how many clusters per mini-batch\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_data = ClusterData(data, num_parts=num_clusters, keep_inter_cluster_edges=True)   \n",
    "    loader = ClusterLoader(cluster_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1056a82d-d4e4-427b-9a8d-6804f8c306e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_edge_loss(z, edge_index, num_neg_samples=5, margin=1.0):\n",
    "    \"\"\"\n",
    "    z: [N, out_dim] node embeddings\n",
    "    edge_index: [2, E] positive edges in this mini-batch\n",
    "    For each positive edge (i,j), sample `num_neg_samples` negative j'.\n",
    "    \"\"\"\n",
    "    \n",
    "    src, dst = edge_index     \n",
    "\n",
    "    pos_score = F.cosine_similarity(z[src], z[dst], dim=1)\n",
    "\n",
    "    E = dst.size(0)\n",
    "    neg_dst = dst[torch.randint(0, E, (E * num_neg_samples,))]\n",
    "\n",
    "    neg_src = src.repeat_interleave(num_neg_samples)   \n",
    "\n",
    "    neg_score = F.cosine_similarity(z[neg_src], z[neg_dst], dim=1)\n",
    "\n",
    "    loss = torch.relu(margin - pos_score.repeat(num_neg_samples) + neg_score)\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def contrastive_loss(z, temperature=0.5):\n",
    "    \"\"\"\n",
    "    z: [N_b, d] embeddings for the batch\n",
    "    Returns NT-Xent loss that pulls each embedding towards itself under \n",
    "    two random augmentations (here we simply treat each node once).\n",
    "    \"\"\"\n",
    "\n",
    "    z_norm = F.normalize(z, p=2, dim=1)                         \n",
    "\n",
    "    sim = torch.matmul(z_norm, z_norm.T) / temperature         \n",
    "\n",
    "    mask = torch.eye(sim.size(0), device=sim.device).bool()\n",
    "    sim.masked_fill_(mask, -9e15)\n",
    "\n",
    "    labels = torch.arange(sim.size(0), device=sim.device)\n",
    "\n",
    "    return F.cross_entropy(sim, labels)                        \n",
    "\n",
    "\n",
    "def joint_loss(z, edge_index):\n",
    "  \n",
    "    loss_hinge = hinge_edge_loss(z, edge_index)\n",
    "\n",
    "    loss_con = contrastive_loss(z)\n",
    "\n",
    "    return loss_hinge + loss_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80e20cc-dbce-4215-958f-7f96340b06c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model(batch.x, batch.edge_index)\n",
    "        loss = joint_loss(z,batch.edge_index)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_nodes\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def extract_embeddings(model, data):\n",
    "    model.eval()                                   \n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():                         \n",
    "        z = model(data.x, data.edge_index)  \n",
    "        \n",
    "    return z.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5eb8b8-55a3-4c9e-8f63-b05cbad3f289",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d70768-f0cc-46c6-87b8-488e6dddc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "output_dim = 64\n",
    "\n",
    "n_neighbors = 15\n",
    "num_clusters = 8\n",
    "\n",
    "epochs = 100\n",
    "lr=0.005\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb90d7-94a1-4fd8-9ea1-6719352edf0e",
   "metadata": {},
   "source": [
    "## GHTM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b420642d-46d9-4827-b525-4c289a03e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=None,\n",
    "    token_pattern=None,\n",
    "    lowercase=False,\n",
    "    tokenizer=bengali_tokenizer,\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "docs_as_strings = [' '.join(doc) for doc in docs]\n",
    "\n",
    "sparse = vectorizer.fit_transform(docs_as_strings)\n",
    "terms = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2236dd5-5754-4a65-bac1-076c713f10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.array([glove.get(t, np.zeros(emb_dim)) for t in terms])\n",
    "doc_embeds = sparse.dot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69e39d88-3a55-4e98-a7d5-fb28a2b67ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12870/3344674824.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor([A.row, A.col], dtype=torch.long)\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9731271859529318400.00\n",
      "Epoch 10, Loss: 9731271859529318400.00\n",
      "Epoch 20, Loss: 9731271859529318400.00\n",
      "Epoch 30, Loss: 9731271859529318400.00\n",
      "Epoch 40, Loss: 9731271859529318400.00\n",
      "Epoch 50, Loss: 9731271859529318400.00\n",
      "Epoch 60, Loss: 9731271859529318400.00\n",
      "Epoch 70, Loss: 9731271859529318400.00\n",
      "Epoch 80, Loss: 9731271859529318400.00\n",
      "Epoch 90, Loss: 9731271859529318400.00\n"
     ]
    }
   ],
   "source": [
    "data = build_knn_graph(doc_embeds, k=n_neighbors)\n",
    "\n",
    "loader = get_cluster_loader(data, num_clusters=num_clusters, batch_size=batch_size)\n",
    "model = ClusterGCNNet(doc_embeds.shape[1], hidden_dim=hidden_dim, out_dim=output_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, loader, optimizer, device)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9f8559-59e8-4825-af8b-60b22931b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = extract_embeddings(model, data)\n",
    "non_neg_embeddings = np.abs(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d36316-deca-4ec0-80d3-21846389b9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "nmf = NMF(n_components=num_topics, random_state=42, init='nndsvda', max_iter=500)\n",
    "doc_topic = nmf.fit_transform(non_neg_embeddings)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05645e77-3c49-474e-94eb-9aa9ee866a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0: ['প্রবৃদ্ধি', 'দেশ', 'উন্নয়ন', 'খাত', 'শতাংশ', 'হার', 'অর্থনৈতিক', 'উন্নয়নশীল', 'জিডিপি', 'কৃষি']\n",
      "Topic  1: ['টাকা', 'ব্যাংক', 'হিসাব', 'নগদ', 'আমানত', 'লেনদেন', 'ট্রেডার্স', 'নগদান', 'প্রাক্কলিত', 'চেক']\n",
      "Topic  2: ['ভিটামিন', 'মাংস', 'মাশরুম', 'খাদ্য', 'উৎস', 'মাছ', 'আটা', 'দ্রবণীয়', 'পরিমাণ', 'খাবার']\n",
      "Topic  3: ['লীগ', 'পাকিস্তান', 'নির্বাচন', 'মুসলিম', 'আওয়ামী', 'দল', 'সাল', 'আসন', 'পরিষদ', 'যুক্তফ্রন্ট']\n",
      "Topic  4: ['কুরআন', 'আল', 'হাদিস', 'নবি', 'আল্লাহ', 'কিতাব', 'রাসুল', 'তায়ালা', 'হযরত', 'ওহি']\n",
      "Topic  5: ['বৃষ্টিপাত', 'তাপমাত্রা', 'বাংলাদেশ', 'বন্যা', 'জলবায়ু', 'বায়ু', 'মৌসুমি', 'সেলসিয়াস', 'শীতকাল', 'বর্ষাকাল']\n",
      "Topic  6: ['ফোন', 'মোবাইল', 'কার্ড', 'প্রজন্ম', 'প্রযুক্তি', 'নেটওয়ার্ক', 'যোগাযোগ', 'সুইচিং', 'ডিভাইস', 'ওয়্যারলেস']\n",
      "Topic  7: ['জাত', 'বীজ', 'ধান', 'ব্রি', 'বপন', 'চারা', 'ফসল', 'চাষ', 'জমি', 'রোপণ']\n"
     ]
    }
   ],
   "source": [
    "topn_words = 10\n",
    "for t in range(num_topics):\n",
    "    best_docs = np.argsort(doc_topic[:, t])[::-1][:10]\n",
    "    \n",
    "    topic_term_weights = np.asarray(sparse[best_docs].sum(axis=0)).ravel()\n",
    "    \n",
    "    top_terms = terms[np.argsort(topic_term_weights)[::-1][:topn_words]]\n",
    "    topic_words.append(top_terms.tolist())\n",
    "    print(f\"Topic {t:>2}:\", top_terms.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b59576-7f0f-439d-a8e8-626b6264c64a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db20fe5-a0d0-42cb-adc1-2c7efd14ff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coherence_c_v': 0.8375407594239899,\n",
       " 'coherence_npmi': 0.24323283050042696,\n",
       " 'topic_diversity': 1.0,\n",
       " 'IRBO': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_topic_words(topic_words, docs)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444396a6-e4eb-41bc-a013-abb7275f755e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
